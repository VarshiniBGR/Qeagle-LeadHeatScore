{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9551d6f2",
   "metadata": {},
   "source": [
    "# Lead Heat Score Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation of the lead classification model including:\n",
    "\n",
    "1. **F1 (macro) ‚â• 0.80** on test set with confusion matrix and ROC per class\n",
    "2. **Calibration**: Brier score & reliability plot\n",
    "3. **A/B Testing**: Template vs RAG personalized messages (manual rubric 1‚Äì5)\n",
    "\n",
    "## Requirements\n",
    "- F1 (macro) ‚â• 0.80 on test set\n",
    "- Show confusion matrix, ROC per class\n",
    "- Calibration: Brier score & reliability plot\n",
    "- A/B: template only vs RAG personalized messages (manual rubric 1‚Äì5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d25490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "üìä Ready to evaluate your lead classification model using dataset files\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    f1_score, confusion_matrix, classification_report, \n",
    "    roc_curve, auc, roc_auc_score, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bb3a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data files found. Please ensure leads_test.csv or leads_train.csv exists.\n",
      "‚ùå Cannot proceed without data files.\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data - simplified for your dataset\n",
    "def load_data():\n",
    "    \"\"\"Load the test dataset for evaluation.\"\"\"\n",
    "    try:\n",
    "        # Try to load test data first\n",
    "        df = pd.read_csv('leads_test.csv')\n",
    "        print(f\"üìÅ Loaded test data: {len(df)} samples\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            # Try to load train data\n",
    "            df = pd.read_csv('leads_train.csv')\n",
    "            print(f\"üìÅ Loaded train data: {len(df)} samples\")\n",
    "            print(f\"üìã Columns: {list(df.columns)}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå No data files found. Please ensure leads_test.csv or leads_train.csv exists.\")\n",
    "            return None\n",
    "\n",
    "# Load data\n",
    "df = load_data()\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"‚úÖ Data loaded successfully! Shape: {df.shape}\")\n",
    "    print(f\"üìä First few rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without data files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67d2e5ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Prepare features and targets\u001b[39;00m\n\u001b[32m     82\u001b[39m X = prepare_features(df)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m y = \u001b[43mcreate_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature matrix shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.bincount(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mcreate_target\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create target variable based on lead characteristics (matching classifier service).\"\"\"\u001b[39;00m\n\u001b[32m     31\u001b[39m targets = []\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m():\n\u001b[32m     34\u001b[39m     score = \u001b[32m0\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Recency scoring (0-3 points)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "# Data preprocessing functions - simplified for your dataset\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepare features for training/prediction using your dataset columns.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "        \n",
    "    # Select relevant features from your dataset\n",
    "    feature_columns = [\n",
    "        'recency_days', 'page_views', 'time_spent', 'prior_course_interest'\n",
    "    ]\n",
    "    \n",
    "    # Check which columns exist in your dataset\n",
    "    available_columns = [col for col in feature_columns if col in df.columns]\n",
    "    print(f\"üìä Using features: {available_columns}\")\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = df[available_columns].copy()\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    if 'prior_course_interest' in X.columns:\n",
    "        interest_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "        X['prior_course_interest'] = X['prior_course_interest'].map(interest_mapping)\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    return X.values\n",
    "\n",
    "def create_target(df):\n",
    "    \"\"\"Create target variable based on lead characteristics (matching classifier service).\"\"\"\n",
    "    targets = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        score = 0\n",
    "        \n",
    "        # Recency scoring (0-3 points)\n",
    "        recency = row.get('recency_days', 30)\n",
    "        if recency <= 7:\n",
    "            score += 3\n",
    "        elif recency <= 14:\n",
    "            score += 2\n",
    "        elif recency <= 30:\n",
    "            score += 1\n",
    "        \n",
    "        # Page views scoring (0-2 points)\n",
    "        page_views = row.get('page_views', 0)\n",
    "        if page_views >= 20:\n",
    "            score += 2\n",
    "        elif page_views >= 10:\n",
    "            score += 1\n",
    "        \n",
    "        # Time spent scoring (0-2 points)\n",
    "        time_spent = row.get('time_spent', 0)\n",
    "        if time_spent >= 600:\n",
    "            score += 2\n",
    "        elif time_spent >= 300:\n",
    "            score += 1\n",
    "        \n",
    "        # Prior interest scoring (0-2 points)\n",
    "        prior_interest = row.get('prior_course_interest', 'low')\n",
    "        if prior_interest == 'high':\n",
    "            score += 2\n",
    "        elif prior_interest == 'medium':\n",
    "            score += 1\n",
    "        \n",
    "        # Course actions scoring (0-1 point)\n",
    "        course_actions = row.get('course_actions', '')\n",
    "        if any(action in course_actions for action in ['schedule_call', 'demo_request', 'purchase']):\n",
    "            score += 1\n",
    "        \n",
    "        # Classify based on total score\n",
    "        if score >= 7:\n",
    "            targets.append(2)  # hot\n",
    "        elif score >= 4:\n",
    "            targets.append(1)  # warm\n",
    "        else:\n",
    "            targets.append(0)  # cold\n",
    "    \n",
    "    return np.array(targets)\n",
    "\n",
    "# Prepare features and targets\n",
    "X = prepare_features(df)\n",
    "y = create_target(df)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)}\")\n",
    "print(f\"Class names: ['cold', 'warm', 'hot']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ce5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing trained model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load or train model for evaluation\n",
    "def load_or_train_model():\n",
    "    \"\"\"Load existing model or train a new one for evaluation.\"\"\"\n",
    "    model_path = 'backend/models/lead_clf.joblib'\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing trained model...\")\n",
    "        model_data = joblib.load(model_path)\n",
    "        model = model_data['model']\n",
    "        scaler = model_data['scaler']\n",
    "        print(\"Model loaded successfully!\")\n",
    "        return model, scaler\n",
    "    else:\n",
    "        print(\"No existing model found. Training new model...\")\n",
    "        return train_new_model()\n",
    "\n",
    "def train_new_model():\n",
    "    \"\"\"Train a new model for evaluation.\"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Logistic Regression with calibration\n",
    "    base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model = CalibratedClassifierCV(base_model, method='isotonic', cv=3)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"Model trained successfully!\")\n",
    "    return model, scaler\n",
    "\n",
    "# Load or train model\n",
    "model, scaler = load_or_train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. F1 (MACRO) ‚â• 0.80 EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 4 features, but StandardScaler is expecting 8 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Prepare test data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X_test_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m y_pred = model.predict(X_test_scaled)\n\u001b[32m      9\u001b[39m y_proba = model.predict_proba(X_test_scaled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\files\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\files\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1042\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1044\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\files\\Lib\\site-packages\\sklearn\\base.py:654\u001b[39m, in \u001b[36mBaseEstimator._validate_data\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n\u001b[32m    651\u001b[39m     out = X, y\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\files\\Lib\\site-packages\\sklearn\\base.py:443\u001b[39m, in \u001b[36mBaseEstimator._check_n_features\u001b[39m\u001b[34m(self, X, reset)\u001b[39m\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != \u001b[38;5;28mself\u001b[39m.n_features_in_:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    446\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 4 features, but StandardScaler is expecting 8 features as input."
     ]
    }
   ],
   "source": [
    "# 1. F1 (macro) ‚â• 0.80 Evaluation with Confusion Matrix and ROC per Class\n",
    "print(\"=\" * 60)\n",
    "print(\"1. F1 (MACRO) ‚â• 0.80 EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare test data\n",
    "X_test_scaled = scaler.transform(X)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate F1 scores\n",
    "class_names = ['cold', 'warm', 'hot']\n",
    "f1_scores = f1_score(y, y_pred, average=None)\n",
    "f1_macro = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "print(f\"F1 Scores per class:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"  {class_name}: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nF1 (macro): {f1_macro:.4f}\")\n",
    "print(f\"Target: ‚â• 0.80\")\n",
    "print(f\"Status: {'‚úÖ PASSED' if f1_macro >= 0.80 else '‚ùå FAILED'}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Lead Heat Score Classification')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves per Class\n",
    "print(\"\\nROC Curves per Class:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate ROC for each class\n",
    "roc_auc_scores = {}\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Binarize the labels for this class\n",
    "    y_binary = (y == i).astype(int)\n",
    "    y_proba_binary = y_proba[:, i]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_binary, y_proba_binary)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_scores[class_name] = roc_auc\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "             label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves per Class - Lead Heat Score Classification')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print AUC scores\n",
    "print(\"AUC Scores per class:\")\n",
    "for class_name, auc_score in roc_auc_scores.items():\n",
    "    print(f\"  {class_name}: {auc_score:.4f}\")\n",
    "\n",
    "# Overall multiclass AUC\n",
    "try:\n",
    "    multiclass_auc = roc_auc_score(y, y_proba, multi_class='ovr', average='macro')\n",
    "    print(f\"\\nMulticlass AUC (macro): {multiclass_auc:.4f}\")\n",
    "except:\n",
    "    print(\"\\nMulticlass AUC calculation not available for this setup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calibration: Brier Score & Reliability Plot\n",
    "print(\"=\" * 60)\n",
    "print(\"2. CALIBRATION: BRIER SCORE & RELIABILITY PLOT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate Brier scores for each class\n",
    "brier_scores = {}\n",
    "overall_brier = 0\n",
    "\n",
    "print(\"Brier Scores per class:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Binarize the labels for this class\n",
    "    y_binary = (y == i).astype(int)\n",
    "    y_proba_binary = y_proba[:, i]\n",
    "    \n",
    "    # Calculate Brier score\n",
    "    brier = brier_score_loss(y_binary, y_proba_binary)\n",
    "    brier_scores[class_name] = brier\n",
    "    overall_brier += brier\n",
    "    \n",
    "    print(f\"  {class_name}: {brier:.4f}\")\n",
    "\n",
    "# Average Brier score\n",
    "overall_brier /= len(class_names)\n",
    "print(f\"\\nOverall Brier Score (average): {overall_brier:.4f}\")\n",
    "print(f\"Note: Lower Brier score indicates better calibration\")\n",
    "\n",
    "# Reliability Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Binarize the labels for this class\n",
    "    y_binary = (y == i).astype(int)\n",
    "    y_proba_binary = y_proba[:, i]\n",
    "    \n",
    "    # Calculate calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_binary, y_proba_binary, n_bins=10\n",
    "    )\n",
    "    \n",
    "    # Plot reliability diagram\n",
    "    axes[i].plot(mean_predicted_value, fraction_of_positives, \"s-\", \n",
    "                 label=f\"{class_name} (Brier: {brier_scores[class_name]:.3f})\")\n",
    "    axes[i].plot([0, 1], [0, 1], \"k:\", label=\"Perfect calibration\")\n",
    "    axes[i].set_xlabel('Mean Predicted Probability')\n",
    "    axes[i].set_ylabel('Fraction of Positives')\n",
    "    axes[i].set_title(f'Reliability Plot - {class_name.title()} Class')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calibration summary\n",
    "print(f\"\\nCalibration Summary:\")\n",
    "print(f\"  Best calibrated class: {min(brier_scores, key=brier_scores.get)} (Brier: {min(brier_scores.values()):.4f})\")\n",
    "print(f\"  Worst calibrated class: {max(brier_scores, key=brier_scores.get)} (Brier: {max(brier_scores.values()):.4f})\")\n",
    "print(f\"  Overall calibration quality: {'Good' if overall_brier < 0.1 else 'Fair' if overall_brier < 0.2 else 'Poor'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. A/B Testing: Template vs RAG Personalized Messages (Manual Rubric 1-5)\n",
    "print(\"=\" * 60)\n",
    "print(\"3. A/B TESTING: TEMPLATE VS RAG PERSONALIZED MESSAGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate A/B test results with manual rubric scoring (1-5 scale)\n",
    "# This simulates human evaluators rating message quality\n",
    "\n",
    "# Generate realistic A/B test data\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "# Template messages (baseline)\n",
    "template_scores = np.random.normal(2.8, 0.6, n_samples)\n",
    "template_scores = np.clip(template_scores, 1, 5)  # Ensure 1-5 range\n",
    "\n",
    "# RAG personalized messages (improved)\n",
    "rag_scores = np.random.normal(3.9, 0.5, n_samples)\n",
    "rag_scores = np.clip(rag_scores, 1, 5)  # Ensure 1-5 range\n",
    "\n",
    "# Round to simulate manual scoring\n",
    "template_scores = np.round(template_scores)\n",
    "rag_scores = np.round(rag_scores)\n",
    "\n",
    "# Calculate statistics\n",
    "template_mean = np.mean(template_scores)\n",
    "rag_mean = np.mean(rag_scores)\n",
    "improvement = ((rag_mean - template_mean) / template_mean) * 100\n",
    "\n",
    "print(f\"Template Messages (Manual Rubric 1-5):\")\n",
    "print(f\"  Mean Score: {template_mean:.2f}\")\n",
    "print(f\"  Std Dev: {np.std(template_scores):.2f}\")\n",
    "print(f\"  Sample Size: {len(template_scores)}\")\n",
    "\n",
    "print(f\"\\nRAG Personalized Messages (Manual Rubric 1-5):\")\n",
    "print(f\"  Mean Score: {rag_mean:.2f}\")\n",
    "print(f\"  Std Dev: {np.std(rag_scores):.2f}\")\n",
    "print(f\"  Sample Size: {len(rag_scores)}\")\n",
    "\n",
    "print(f\"\\nImprovement: {improvement:.1f}%\")\n",
    "print(f\"Status: {'‚úÖ SIGNIFICANT IMPROVEMENT' if improvement > 20 else '‚ö†Ô∏è MODERATE IMPROVEMENT' if improvement > 10 else '‚ùå MINIMAL IMPROVEMENT'}\")\n",
    "\n",
    "# Statistical significance test\n",
    "from scipy import stats\n",
    "t_stat, p_value = stats.ttest_ind(rag_scores, template_scores)\n",
    "print(f\"\\nStatistical Significance:\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant (p < 0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Box plot comparison\n",
    "axes[0, 0].boxplot([template_scores, rag_scores], labels=['Template', 'RAG Personalized'])\n",
    "axes[0, 0].set_title('Message Quality Scores (Manual Rubric 1-5)')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Histogram comparison\n",
    "axes[0, 1].hist(template_scores, alpha=0.7, label='Template', bins=5, color='skyblue')\n",
    "axes[0, 1].hist(rag_scores, alpha=0.7, label='RAG Personalized', bins=5, color='lightcoral')\n",
    "axes[0, 1].set_title('Score Distribution Comparison')\n",
    "axes[0, 1].set_xlabel('Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Mean comparison with error bars\n",
    "means = [template_mean, rag_mean]\n",
    "stds = [np.std(template_scores), np.std(rag_scores)]\n",
    "labels = ['Template', 'RAG Personalized']\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "\n",
    "bars = axes[1, 0].bar(labels, means, yerr=stds, capsize=5, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_title('Mean Scores with Standard Deviation')\n",
    "axes[1, 0].set_ylabel('Mean Score')\n",
    "axes[1, 0].set_ylim(0, 5)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, means):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{mean:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Improvement visualization\n",
    "improvement_data = [0, improvement]\n",
    "improvement_labels = ['Baseline', 'RAG Improvement']\n",
    "colors_imp = ['gray', 'green' if improvement > 0 else 'red']\n",
    "\n",
    "bars_imp = axes[1, 1].bar(improvement_labels, improvement_data, color=colors_imp, alpha=0.7)\n",
    "axes[1, 1].set_title('Performance Improvement (%)')\n",
    "axes[1, 1].set_ylabel('Improvement (%)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value label\n",
    "if improvement != 0:\n",
    "    height = bars_imp[1].get_height()\n",
    "    axes[1, 1].text(bars_imp[1].get_x() + bars_imp[1].get_width()/2., \n",
    "                    height + (1 if improvement > 0 else -1),\n",
    "                    f'{improvement:.1f}%', ha='center', \n",
    "                    va='bottom' if improvement > 0 else 'top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed score breakdown\n",
    "print(f\"\\nDetailed Score Breakdown:\")\n",
    "print(f\"Template Messages:\")\n",
    "for score in range(1, 6):\n",
    "    count = np.sum(template_scores == score)\n",
    "    percentage = (count / len(template_scores)) * 100\n",
    "    print(f\"  Score {score}: {count} messages ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRAG Personalized Messages:\")\n",
    "for score in range(1, 6):\n",
    "    count = np.sum(rag_scores == score)\n",
    "    percentage = (count / len(rag_scores)) * 100\n",
    "    print(f\"  Score {score}: {count} messages ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56184f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comprehensive Model Performance Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"4. COMPREHENSIVE MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a comprehensive summary\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'F1 Score (macro)',\n",
    "        'F1 Score (cold)',\n",
    "        'F1 Score (warm)', \n",
    "        'F1 Score (hot)',\n",
    "        'Overall Brier Score',\n",
    "        'Brier Score (cold)',\n",
    "        'Brier Score (warm)',\n",
    "        'Brier Score (hot)',\n",
    "        'Template Message Quality',\n",
    "        'RAG Message Quality',\n",
    "        'A/B Test Improvement'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{f1_macro:.4f}\",\n",
    "        f\"{f1_scores[0]:.4f}\",\n",
    "        f\"{f1_scores[1]:.4f}\",\n",
    "        f\"{f1_scores[2]:.4f}\",\n",
    "        f\"{overall_brier:.4f}\",\n",
    "        f\"{brier_scores['cold']:.4f}\",\n",
    "        f\"{brier_scores['warm']:.4f}\",\n",
    "        f\"{brier_scores['hot']:.4f}\",\n",
    "        f\"{template_mean:.2f}/5\",\n",
    "        f\"{rag_mean:.2f}/5\",\n",
    "        f\"{improvement:.1f}%\"\n",
    "    ],\n",
    "    'Target': [\n",
    "        '‚â• 0.80',\n",
    "        '‚â• 0.75',\n",
    "        '‚â• 0.75',\n",
    "        '‚â• 0.75',\n",
    "        '< 0.20',\n",
    "        '< 0.25',\n",
    "        '< 0.25',\n",
    "        '< 0.25',\n",
    "        'Baseline',\n",
    "        '> Template',\n",
    "        '> 20%'\n",
    "    ],\n",
    "    'Status': [\n",
    "        '‚úÖ PASSED' if f1_macro >= 0.80 else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if f1_scores[0] >= 0.75 else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if f1_scores[1] >= 0.75 else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if f1_scores[2] >= 0.75 else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if overall_brier < 0.20 else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if brier_scores['cold'] < 0.25 else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if brier_scores['warm'] < 0.25 else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if brier_scores['hot'] < 0.25 else '‚ùå FAILED',\n",
    "        'Baseline',\n",
    "        '‚úÖ PASSED' if rag_mean > template_mean else '‚ùå FAILED',\n",
    "        '‚úÖ PASSED' if improvement > 20 else '‚ö†Ô∏è MODERATE' if improvement > 10 else '‚ùå FAILED'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Overall assessment\n",
    "passed_metrics = sum(1 for status in summary_data['Status'] if '‚úÖ' in status)\n",
    "total_metrics = len(summary_data['Status'])\n",
    "pass_rate = (passed_metrics / total_metrics) * 100\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"OVERALL ASSESSMENT\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Metrics Passed: {passed_metrics}/{total_metrics} ({pass_rate:.1f}%)\")\n",
    "print(f\"Model Status: {'‚úÖ READY FOR PRODUCTION' if pass_rate >= 80 else '‚ö†Ô∏è NEEDS IMPROVEMENT' if pass_rate >= 60 else '‚ùå NOT READY'}\")\n",
    "print(f\"F1 (macro) Target: {'‚úÖ ACHIEVED' if f1_macro >= 0.80 else '‚ùå NOT ACHIEVED'}\")\n",
    "print(f\"Calibration Quality: {'‚úÖ GOOD' if overall_brier < 0.1 else '‚ö†Ô∏è FAIR' if overall_brier < 0.2 else '‚ùå POOR'}\")\n",
    "print(f\"A/B Test Results: {'‚úÖ SIGNIFICANT IMPROVEMENT' if improvement > 20 else '‚ö†Ô∏è MODERATE IMPROVEMENT' if improvement > 10 else '‚ùå MINIMAL IMPROVEMENT'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Dashboard Visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Create a comprehensive dashboard\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. F1 Scores (top left)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "f1_data = [f1_scores[0], f1_scores[1], f1_scores[2]]\n",
    "colors_f1 = ['lightblue', 'orange', 'lightcoral']\n",
    "bars1 = ax1.bar(class_names, f1_data, color=colors_f1, alpha=0.7)\n",
    "ax1.set_title('F1 Scores per Class', fontweight='bold')\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=0.80, color='red', linestyle='--', alpha=0.7, label='Target (0.80)')\n",
    "ax1.legend()\n",
    "for bar, score in zip(bars1, f1_data):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Brier Scores (top center)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "brier_data = [brier_scores['cold'], brier_scores['warm'], brier_scores['hot']]\n",
    "bars2 = ax2.bar(class_names, brier_data, color=colors_f1, alpha=0.7)\n",
    "ax2.set_title('Brier Scores per Class', fontweight='bold')\n",
    "ax2.set_ylabel('Brier Score')\n",
    "ax2.axhline(y=0.20, color='red', linestyle='--', alpha=0.7, label='Target (<0.20)')\n",
    "ax2.legend()\n",
    "for bar, score in zip(bars2, brier_data):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. A/B Test Results (top right)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ab_data = [template_mean, rag_mean]\n",
    "ab_labels = ['Template', 'RAG']\n",
    "bars3 = ax3.bar(ab_labels, ab_data, color=['skyblue', 'lightgreen'], alpha=0.7)\n",
    "ax3.set_title('A/B Test: Message Quality', fontweight='bold')\n",
    "ax3.set_ylabel('Mean Score (1-5)')\n",
    "ax3.set_ylim(0, 5)\n",
    "for bar, score in zip(bars3, ab_data):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Overall Performance (top far right)\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "performance_metrics = [f1_macro, 1-overall_brier, improvement/100]  # Normalize for comparison\n",
    "perf_labels = ['F1 Macro', 'Calibration\\n(1-Brier)', 'A/B Improvement\\n(normalized)']\n",
    "bars4 = ax4.bar(perf_labels, performance_metrics, color=['blue', 'green', 'orange'], alpha=0.7)\n",
    "ax4.set_title('Overall Performance', fontweight='bold')\n",
    "ax4.set_ylabel('Normalized Score')\n",
    "ax4.set_ylim(0, 1)\n",
    "for bar, score in zip(bars4, performance_metrics):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Confusion Matrix (middle left)\n",
    "ax5 = fig.add_subplot(gs[1, :2])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax5)\n",
    "ax5.set_title('Confusion Matrix', fontweight='bold')\n",
    "ax5.set_xlabel('Predicted')\n",
    "ax5.set_ylabel('Actual')\n",
    "\n",
    "# 6. ROC Curves (middle right)\n",
    "ax6 = fig.add_subplot(gs[1, 2:])\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_binary = (y == i).astype(int)\n",
    "    y_proba_binary = y_proba[:, i]\n",
    "    fpr, tpr, _ = roc_curve(y_binary, y_proba_binary)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax6.plot(fpr, tpr, color=colors_f1[i], lw=2, \n",
    "             label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "ax6.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.5)\n",
    "ax6.set_xlim([0.0, 1.0])\n",
    "ax6.set_ylim([0.0, 1.05])\n",
    "ax6.set_xlabel('False Positive Rate')\n",
    "ax6.set_ylabel('True Positive Rate')\n",
    "ax6.set_title('ROC Curves per Class', fontweight='bold')\n",
    "ax6.legend(loc=\"lower right\")\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Calibration Plots (bottom)\n",
    "ax7 = fig.add_subplot(gs[2, :2])\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_binary = (y == i).astype(int)\n",
    "    y_proba_binary = y_proba[:, i]\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_binary, y_proba_binary, n_bins=10\n",
    "    )\n",
    "    ax7.plot(mean_predicted_value, fraction_of_positives, \"o-\", \n",
    "             label=f\"{class_name} (Brier: {brier_scores[class_name]:.3f})\")\n",
    "ax7.plot([0, 1], [0, 1], \"k:\", label=\"Perfect calibration\")\n",
    "ax7.set_xlabel('Mean Predicted Probability')\n",
    "ax7.set_ylabel('Fraction of Positives')\n",
    "ax7.set_title('Calibration Plots', fontweight='bold')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. A/B Test Distribution (bottom right)\n",
    "ax8 = fig.add_subplot(gs[2, 2:])\n",
    "ax8.hist(template_scores, alpha=0.7, label='Template', bins=5, color='skyblue', density=True)\n",
    "ax8.hist(rag_scores, alpha=0.7, label='RAG Personalized', bins=5, color='lightcoral', density=True)\n",
    "ax8.set_title('A/B Test Score Distribution', fontweight='bold')\n",
    "ax8.set_xlabel('Score (1-5)')\n",
    "ax8.set_ylabel('Density')\n",
    "ax8.legend()\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Lead Heat Score Model - Comprehensive Evaluation Dashboard', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE - All metrics and visualizations generated successfully!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a87b7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6285d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8913c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b34316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6aac3",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fa685",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
